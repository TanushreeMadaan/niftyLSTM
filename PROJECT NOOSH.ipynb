{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19271d4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.17' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.10 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import pandas_ta\n",
    "import warnings\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "nifty50_tickers = [\n",
    "    \"ASIANPAINT.NS\", \"BRITANNIA.NS\", \"CIPLA.NS\", \"EICHERMOT.NS\", \"NESTLEIND.NS\",\n",
    "        \"GRASIM.NS\", \"HEROMOTOCO.NS\", \"HINDALCO.NS\", \"HINDUNILVR.NS\", \"ITC.NS\",\n",
    "            \"LT.NS\", \"M&M.NS\", \"RELIANCE.NS\", \"TATACONSUM.NS\", \"TATAMOTORS.NS\",\n",
    "                \"TATASTEEL.NS\", \"WIPRO.NS\", \"APOLLOHOSP.NS\", \"DRREDDY.NS\", \"TITAN.NS\",\n",
    "                    \"SBIN.NS\", \"SHRIRAMFIN.NS\", \"BPCL.NS\", \"KOTAKBANK.NS\", \"INFY.NS\",\n",
    "                        \"BAJFINANCE.NS\", \"ADANIENT.NS\", \"SUNPHARMA.NS\", \"JSWSTEEL.NS\", \"HDFCBANK.NS\",\n",
    "                            \"TCS.NS\", \"ICICIBANK.NS\", \"POWERGRID.NS\", \"MARUTI.NS\", \"INDUSINDBK.NS\",\n",
    "                                \"AXISBANK.NS\", \"HCLTECH.NS\", \"ONGC.NS\", \"NTPC.NS\", \"COALINDIA.NS\",\n",
    "                                    \"BHARTIARTL.NS\", \"TECHM.NS\", \"LTIM.NS\", \"DIVISLAB.NS\", \"ADANIPORTS.NS\",\n",
    "                                        \"HDFCLIFE.NS\", \"SBILIFE.NS\", \"ULTRACEMCO.NS\"\n",
    "                                        ]\n",
    "\n",
    "# Date Range(10 years time period)\n",
    "end_date = '2024-07-23'\n",
    "start_date = pd.to_datetime('2014-01-01')\n",
    "\n",
    "# Dataset-Yfinance\n",
    "df = yf.download(tickers=nifty50_tickers,\n",
    "                            start=start_date,\n",
    "                             end=end_date).stack()\n",
    "\n",
    "# Rename the index names for the dataframe(multi index dataframe)\n",
    "df.index.names = ['date', 'ticker']\n",
    "df.columns = df.columns.str.lower()\n",
    "df\n",
    "#This dataframe is at date level(contains daily stock prices of nifty 50 stocks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4488b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING(techincal indicators) using pandas_ta lib\n",
    "\n",
    "# 1.Garman-Klass Volatility\n",
    "# Formula: 0.5 * (log(High) - log(Low))^2 - (2*log(2)-1) * (log(Close) - log(Open))^2\n",
    "df['garman_klass_vol'] = (\n",
    "    ((np.log(df['high']) - np.log(df['low'])) ** 2) / 2 -\n",
    "    (2 * np.log(2) - 1) * ((np.log(df['adj close']) - np.log(df['open'])) ** 2)\n",
    ")\n",
    "\n",
    "#2. RSI (Relative Strength Index) calculation for a window of 20 days \n",
    "df['rsi'] = df.groupby(level=1)['adj close'].transform(\n",
    "    lambda x: pandas_ta.rsi(close=x, length=20)\n",
    ")\n",
    "\n",
    "#3. Bollinger Bands for a window of 20 days\n",
    "# bb_low: Lower Bollinger Band\n",
    "df['bb_low'] = df.groupby(level=1)['adj close'].transform(\n",
    "    lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:, 0]\n",
    ")\n",
    "# bb_mid: Middle Bollinger Band (SMA)\n",
    "df['bb_mid'] = df.groupby(level=1)['adj close'].transform(\n",
    "    lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:, 1]\n",
    ")\n",
    "# bb_high: Upper Bollinger Band\n",
    "df['bb_high'] = df.groupby(level=1)['adj close'].transform(\n",
    "    lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:, 2]\n",
    ")\n",
    "\n",
    "#4. ATR (Average True Range): Function to compute ATR, then normalize by subtracting mean and dividing by standard deviation\n",
    "def compute_atr(stock_data):\n",
    "    atr = pandas_ta.atr(\n",
    "        high=stock_data['high'],\n",
    "        low=stock_data['low'],\n",
    "        close=stock_data['close'],\n",
    "        length=14\n",
    "    )\n",
    "    return atr.sub(atr.mean()).div(atr.std())\n",
    "\n",
    "df['atr'] = df.groupby(level=1, group_keys=False).apply(compute_atr)\n",
    "\n",
    "#5. MACD (Moving Average Convergence Divergence): Function to compute MACD, then normalize by subtracting mean and dividing by standard deviation\n",
    "def compute_macd(close):\n",
    "    macd = pandas_ta.macd(close=close, length=20).iloc[:, 0]\n",
    "    return macd.sub(macd.mean()).div(macd.std())\n",
    "\n",
    "df['macd'] = df.groupby(level=1, group_keys=False)['adj close'].apply(compute_macd)\n",
    "\n",
    "#5. Traded Value in crores inr: Calculated as (Adjusted Close Price * Volume) / 1 crore\n",
    "df['traded_value'] = (df['adj close'] * df['volume']) / 1e7\n",
    "\n",
    "# Display the resulting dataframe\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to monthly level \n",
    "\n",
    "# Create a list of column names, excluding those related to volume, price levels, and some others.\n",
    "last_cols = [c for c in df.columns.unique(0)\n",
    "             if c not in ['traded_value', 'volume', 'open', 'high', 'low', 'close']]\n",
    "\n",
    "# 1. Converting the daily 'traded_value' data to monthly frequency by taking the mean.\n",
    "# 2. Convert the rest of the daily data (from last_cols) to monthly frequency by taking the last value of the month(i.e value at last date of the month).\n",
    "\n",
    "data = (pd.concat([\n",
    "    df.unstack('ticker')['traded_value'].resample('M').mean().stack('ticker').to_frame('traded_value'),\n",
    "    df.unstack()[last_cols].resample('M').last().stack('ticker')\n",
    "], axis=1)).dropna()\n",
    "\n",
    "# Display the aggregated monthly data.\n",
    "data\n",
    "# data dataframe is at monthly level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ab2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating the 2-year rolling average of tradeed value for each stock.(2 yr rolling window with minimum of 12 months to calculate mean)\n",
    "\n",
    "\n",
    "data['traded_value'] = (data.loc[:, 'traded_value'].unstack('ticker')\n",
    "                         .rolling(2*12, min_periods=12).mean()\n",
    "                         .stack())\n",
    "\n",
    "# Calculate Monthly Returns for different time horizons as features.\n",
    "\n",
    "# function to calculate monthly returns for various lag periods(1,2,3,6,9,12)\n",
    "# also handles outliers by clipping return values outside a certain range.\n",
    "def calculate_returns(df):\n",
    "\n",
    "    # Define an outlier cutoff threshold. Returns beyond this threshold (both on the low and high side) will be treated as outliers.\n",
    "    outlier_cutoff = 0.005\n",
    "\n",
    "    # Define different time horizons (in months) for which we want to calculate returns.\n",
    "    lags = [1, 2, 3, 6, 9, 12]\n",
    "\n",
    "    # Loop over each lag to calculate the return for that time horizon.\n",
    "    for lag in lags:\n",
    "\n",
    "        df[f'returns_{lag}m'] = (df['adj close']\n",
    "                              .pct_change(lag)\n",
    "                              .pipe(lambda x: x.clip(lower=x.quantile(outlier_cutoff),\n",
    "                                                     upper=x.quantile(1-outlier_cutoff)))\n",
    "                              .add(1)\n",
    "                              .pow(1/lag)\n",
    "                              .sub(1))\n",
    "    return df\n",
    "# Apply the 'calculate_returns' function to our data grouped by ticker (level=1).\n",
    "# Drop rows with NaN values after the calculation.\n",
    "data = data.groupby(level=1, group_keys=False).apply(calculate_returns).dropna()\n",
    "\n",
    "# Display the resulting dataframe with returns for each specified time horizon.\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2bf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af48642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib  # For saving scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd96355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if isinstance(data.index, pd.MultiIndex):\n",
    "    data = data.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE SELECTION\n",
    "features = [\n",
    "    'traded_value', 'atr', 'bb_high', 'bb_low', 'bb_mid',\n",
    "    'garman_klass_vol', 'macd', 'rsi',\n",
    "    'returns_1m', 'returns_2m', 'returns_3m',\n",
    "    'returns_6m', 'returns_9m', 'returns_12m'\n",
    "]\n",
    "target = 'adj close'\n",
    "\n",
    "# SCALLING THE FEATURES\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "data[features] = feature_scaler.fit_transform(data[features])\n",
    "\n",
    "# Shift the target column to predict the next month's closing price\n",
    "data[target] = data.groupby('ticker')[target].shift(-1)\n",
    "\n",
    "# Drop any NaN values that may have resulted from the shift\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Split into features and target variable\n",
    "X = data[features].values\n",
    "y = data[target].values.reshape(-1, 1)  # Reshape for scaler\n",
    "\n",
    "# Scale the target variable\n",
    "y_scaled = target_scaler.fit_transform(y)\n",
    "\n",
    "print(\"\\nFeatures and Target Scaled:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c76eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of time steps (e.g., 12 months)(this will be used in lstm model)\n",
    "time_steps = 12\n",
    "\n",
    "X_lstm = []\n",
    "y_lstm = []\n",
    "tickers = data['ticker'].unique()\n",
    "\n",
    "for ticker in tickers:\n",
    "    ticker_data = data[data['ticker'] == ticker].sort_values('date')\n",
    "    for i in range(time_steps, len(ticker_data)):\n",
    "        X_lstm.append(X[ticker_data.index[i - time_steps:i]])\n",
    "        y_lstm.append(y_scaled[ticker_data.index[i]])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_lstm, y_lstm = np.array(X_lstm), np.array(y_lstm)\n",
    "\n",
    "print(f\"Shape of X_lstm: {X_lstm.shape}\")  # Expected: (samples, time_steps, features)\n",
    "print(f\"Shape of y_lstm: {y_lstm.shape}\")  # Expected: (samples, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92aaf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train-test split ratio\n",
    "test_size = 0.2  # 20% for testing\n",
    "split_index = int(X_lstm.shape[0] * (1 - test_size))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test = X_lstm[:split_index], X_lstm[split_index:]\n",
    "y_train, y_test = y_lstm[:split_index], y_lstm[split_index:]\n",
    "\n",
    "print(f\"Training Samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing Samples: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a464535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(100, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    LSTM(100, return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with a suitable optimizer and loss function\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  \n",
    "    batch_size=32,\n",
    "    validation_split=0.2,  \n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b766aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions and actual values to their original scale\n",
    "predictions_actual = target_scaler.inverse_transform(predictions)\n",
    "y_test_actual = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test_actual, predictions_actual)\n",
    "mse = mean_squared_error(y_test_actual, predictions_actual)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_actual, predictions_actual)\n",
    "mape = mean_absolute_percentage_error(y_test_actual, predictions_actual)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9056ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Ensure the data is 1D\n",
    "y_test_actual_1d = y_test_actual.ravel() if len(y_test_actual.shape) > 1 else y_test_actual\n",
    "predictions_actual_1d = predictions_actual.ravel() if len(predictions_actual.shape) > 1 else predictions_actual\n",
    "\n",
    "# Prepare the data\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual Closing Price': y_test_actual_1d,\n",
    "    'Predicted Closing Price': predictions_actual_1d\n",
    "})\n",
    "import cufflinks as cf\n",
    "cf.go_offline() \n",
    "\n",
    "# Plot using cufflinks with solid lines and better labels\n",
    "predictions_df.iplot(kind='line',\n",
    "                     title='Actual vs Predicted Closing Prices',\n",
    "                     xTitle='Time',\n",
    "                     yTitle='Closing Price (in Rs)',\n",
    "                     colors=['blue', 'red'],\n",
    "                     theme='white',\n",
    "                     mode='lines',  \n",
    "                     linewidth=3,  \n",
    "                     asFigure=False,\n",
    "                     legend=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b925921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92057c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f7a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b476d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e6d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
